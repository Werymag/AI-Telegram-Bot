services:
  ollama_tg:
    image: ollama/ollama:latest
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    container_name: ollama_tg
    pull_policy: always # Всегда тянуть свежий образ
    tty: true # Необходимо для ollama run
    restart: always
    networks: # Добавляем сервис в сеть
      - ai_network
    # Эта команда запустит модель при старте контейнера Ollama.
    # Если модель не скачана, она будет скачана автоматически.
    # Сначала запускаем сервер в фоне, ждем пока он ответит, потом скачиваем модель и ждем завершения фонового процесса
    entrypoint: ["sh", "-c"]
    # запустить сервер в фоновом режиме, подождать немного (или пока сервер не ответит), а затем выполнить команду pull.
    command: ['ollama serve & echo "Waiting for Ollama server..." && while ! ollama list > /dev/null 2>&1; do sleep 1; done && echo "Ollama ready. Pulling model..." && ollama pull gemma3:12b && echo "Pull complete. Server running." && wait']

  app:
    build: ./tg_ai_bot # Собираем образ из Dockerfile в текущем каталоге
    container_name: python_telegram_bot
    # Передаем переменную окружения с адресом Ollama
    environment:
      - OLLAMA_HOST=http://ollama_tg:11434 # Используем имя сервиса вместо localhost
      - TELEGRAM_TOKEN=5783330360:AAGd5cok3mhB0_iAIF6mP4FLM4Nf5lpiZC0 # Раскомментируй, если хочешь передавать токен так
      - MODEL_NAME=gemma3:12b
      - PYTHONUNBUFFERED=1
    volumes: # Добавляем монтирование папки логов
      - ./logs:/app/logs
    depends_on:
      ollama_tg:
        condition: service_started # Ждем, пока Ollama запустится
    restart: always
    networks: # Добавляем сервис в сеть
      - ai_network

volumes:
  ollama_data:

networks: # Определяем сеть
  ai_network:
    driver: bridge 